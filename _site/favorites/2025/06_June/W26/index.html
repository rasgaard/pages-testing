<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Courses on Quantization and Democratization of AI</title>
    <link rel="stylesheet" href="/styles.css">
    
    
</head>

<body>
    <nav class="site-nav">
        <a href="/" class="site-name">Rasmus Aagaard</a>
        <div class="nav-links">
            <a href="/phd/status/">PhD</a>
            <a href="/blog/">Blog</a>
            <!--<a href="/favorites/">Favorites</a>-->
        </div>
    </nav>
    <main>
        <article>
            <h4>DeepLearning.AI Courses</h4>
<blockquote>
<p><a href="https://learn.deeplearning.ai">https://learn.deeplearning.ai</a></p>
</blockquote>
<p>As I will be starting an industrial ph.d. soon on the topic of model compression and edge deployment I thought it would be nice to familiarise myself a bit more practically with model compression. I sought out courses and found some on <a href="deeplearning.ai">deeplearning.ai</a>. They have one on <a href="https://learn.deeplearning.ai/courses/quantization-fundamentals"><em>Fundamentals</em></a> and a successor that goes a bit more <a href="https://learn.deeplearning.ai/courses/quantization-in-depth"><em>in Depth</em></a>.</p>
<p>I liked the first one as an approachable introduction to the subject. It was also pretty short so it was an easy investment to make. One thing that I miss is that you don't really have to be actively engaged. It would be nice with some more hands-on exercises that forces you to think through the concepts.</p>
<p>I'll be on the lookout for more practical exercises throughout the summer to get more familiar with quantization, pruning and knowledge distillation.</p>
<h4>How to Democratize AI - Peter Wang</h4>
<blockquote>
<p><a href="https://youtu.be/TLZ9zXnluc8">https://youtu.be/TLZ9zXnluc8</a></p>
</blockquote>
<p><a href="https://www.futo.org/">FUTO</a> recently appeared in my YouTube recommendations and I think that the algorithm is pretty spot-on in this case. It's an organization that works against the pattern of computers being users of people, and not the other way around.
An interesting point during the talk by Peter Wang is that since it has been shown that the internal representations of knowledge inside language models all converge, it could be argued that the more we train these models the more it should be viewed as public, common knowledge. Much like how digits of Pi isn't intellectual prep</p>

        </article>
    </main>
    
    
    <script>
        // Theme toggle functionality
        const theme = localStorage.getItem('theme') || 'light';
        document.documentElement.setAttribute('data-theme', theme);

        function toggleTheme() {
            const current = document.documentElement.getAttribute('data-theme');
            const next = current === 'light' ? 'dark' : 'light';
            document.documentElement.setAttribute('data-theme', next);
            localStorage.setItem('theme', next);
        }

        // Add keyboard shortcut (Ctrl+T or Cmd+T)
        document.addEventListener('keydown', (e) => {
            if ((e.ctrlKey || e.metaKey) && e.key === 't') {
                e.preventDefault();
                toggleTheme();
            }
        });
    </script>
</body>

</html>