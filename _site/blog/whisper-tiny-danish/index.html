<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-tuning Whisper-tiny for Danish (for free)</title>
    <link rel="stylesheet" href="/styles.css">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    
    
</head>

<body>
    <nav class="site-nav">
        <a href="/" class="site-name">Rasmus Aagaard</a>
        <div class="nav-links">
            <a href="/phd/status/">PhD</a>
            <a href="/blog/">Blog</a>
            <!--<a href="/favorites/">Favorites</a>-->
        </div>
    </nav>
    <main>
        <article>
            <h1>Fine-tuning Whisper-tiny for Danish (for free)</h1>
<h2>Appreciation for Small Models and Open Datasets</h2>
<p>I have a long-standing appreciation for small yet capable AI models. Being lightweight yet highly performant is so impressive to me. Constraints on model size go a little bit against the grain for state-of-the-art performance as we are constantly witnessing scale being the predominant factor for success. I think this contributes to my fascination. Small models have a certain &quot;underdog&quot; quality to them. Additionally, I believe that putting such constraints on AI systems can result in more interesting solutions.</p>
<p>It can be hard to scratch the model training itch without breaking the bank if you are just a hobbyist with limited computational resources. That's the case for larger models, at least. It is entirely possible to use free resources on Google Colab or Kaggle to train smaller models without ever picking up your credit card. Realizing this got me excited, as it opens the door for hobbyists and tinkerers to explore the space, investing only their time and attention.</p>
<p>The models are only half of the story. Data needs to be accessible too. Projects like <a href="https://alexandra.dk/coral/">CoRal</a> by the Alexandra Institute have done an amazing job at collecting high-quality datasets that are available for everyone. It is also highly beneficial to everyone. Having open datasets puts them under public scrutiny and ensures their quality, ultimately leading to better AI systems.</p>
<h2>Whisper-tiny.da: Putting the pieces together</h2>
<p>I have been looking for an excuse to play around more with Whisper lately. The <code>tiny</code> variant with its 40M parameter size allows it to run smoothly on mobile phones and perform real-time transcription in the browser using <a href="https://huggingface.co/spaces/Xenova/whisper-webgpu">Transformers.js</a>.</p>
<p>The question that I wanted to answer was: &quot;What performance can I squeeze out of Whisper-tiny if trained on high-quality Danish speech?&quot;. I wanted to know where on this <a href="https://huggingface.co/CoRal-project/roest-wav2vec2-315m-v1#evaluation-results">Word/Character Error Rate table</a> it would land.</p>
<p>Luckily for me, a lot of the hard work was already done by other people and packaged up nicely. I read through a <a href="https://huggingface.co/blog/fine-tune-whisper">tutorial post</a> and copied relevant snippets over to a Kaggle notebook. Kaggle offers 30 GPU hours per week which turned out to be plenty for this small project. I later found that the team behind CoRal had put together <a href="https://github.com/alexandrainst/coral">training and evaluation scripts</a> for this exact purpose, making the whole process incredibly easy. I would have cheated myself of the (painful) experiences and lessons of debugging a whole mess of special tokens that got tangled up in the process of stitching together code had I used their code.</p>
<p>After having all the pieces in place — streaming dataset, data collator, training arguments, evaluation code — it was finally time to press Execute Call on <code>trainer.train()</code> and hopefully watch the Word Error Rate (WER) go down.</p>
<h2>How did it do then?</h2>
<p>Here are the results:</p>
<ul>
<li>
<p>Character Error Rate (CER): 15.93%</p>
</li>
<li>
<p>Word Error Rate (WER): 34.30%</p>
</li>
</ul>
<p>This would indicate that it performs similarly as models that are <strong>40x</strong> the size. Wow, that's amazing! ...right?</p>
<p>Well, let's not get ahead of ourselves. Let's try to look at an actual transcription.</p>
<p>To try it out I'll start up a temporary Jupyter server with uv:</p>
<pre><code class="language-bash">&gt; uv run --with ipython --with jupyter --with transformers --with torch
</code></pre>
<p>and copy the Jupyter server URL into VS Code. I find it to be a nice way to create minimal throwaway/temporary environments for testing for one-off work.</p>
<p>I like to test on samples that are not in the original dataset — not even the test set — to set expectations for actual usage. The CoRal project has another <a href="https://huggingface.co/datasets/alexandrainst/coral-tts">dataset for training Text-to-Speech</a> models.</p>
<p>My favorite sample from the CoRal-TTS dataset has the voice actor say &quot;Jeg fucking elsker tebirkes[^1]&quot;.</p>
<p>Let's first benchmark against the original <a href="https://huggingface.co/openai/whisper-tiny"><code>whisper-tiny</code></a> model from OpenAI</p>
<pre><code class="language-python">from transformers import pipeline

audio_link = &quot;https://...&quot;
pipe = pipeline(task=&quot;automatic-speech-recognition&quot;,
model=&quot;openai/whisper-tiny&quot;)
pipe(audio_link)
&gt;&gt;&gt; {'text': ' Ja, fucking, ells godt, tæt biogis.'}
</code></pre>
<p>and then our CoRal fine-tuned <a href="https://huggingface.co/rasgaard/whisper-tiny.da/">whisper-tiny.da</a></p>
<pre><code class="language-python">pipe = pipeline(task=&quot;automatic-speech-recognition&quot;,
model=&quot;rasgaard/whisper-tiny.da&quot;)
pipe(audio_link)
&gt;&gt;&gt; {'text': 'erforking elsker 10 birkes'}
</code></pre>
<p>And finally comparing it to <a href="https://huggingface.co/syvai/hviske-v2"><code>syvai/hviske2</code></a>:</p>
<pre><code class="language-python">pipe = pipeline(task=&quot;automatic-speech-recognition&quot;,
model=&quot;syvai/hviske-v2&quot;)
pipe(audio_link)
&gt;&gt;&gt; {'text': 'jeg fucking elsker tebirkes'}
</code></pre>
<p>...and we can see that there's still some way to go for state-of-the-art performance.</p>
<p>But that's not really what I care about here. I care about <em>best-in-class</em> performance. While it is apparent that there is a long way to go for any kind of <em>good</em> or even <em>slightly useful</em> performance I think it is really interesting to pursue.</p>
<h2>What's next?</h2>
<p>There are a few things that I see as immediate next steps:</p>
<ul>
<li>
<p>More data. I'd love to try out even training on more data to see if generalization across datasets could be improved. A dataset that I can immediately try is <a href="https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0/">Common Voice by Mozilla</a>. I'll get around to that someday soon.</p>
</li>
<li>
<p>Another thing I have considered is that the model doesn't produce any punctuation and only uses lower-case. This is due to the way the CoRal dataset was collected. I have thought about doing some LLM-processing and inserting punctuation and casing into the text in CoRal.</p>
</li>
<li>
<p>Lastly, I'd like to create something useful with the model. Transcription services are highly useful and can even be quite a good <a href="https://goodtape.io/">business case</a>. Having decent transcriptions without the need to send the audio off to a server could enable more use cases that are constrained on privacy. It also allows transcriptions to be done directly on low-power devices such as phones.</p>
</li>
</ul>
<p>I hope to continue this project and extend it into something bigger. It has been fun to have a hobby project where I tried to work within the limitations of what is possible to do when you are GPU-poor and rely on free/openly available services and datasets.</p>
<p>Reach out to me on <a href="https://www.linkedin.com/in/rasgaard/">LinkedIn</a> or <a href="https://bsky.app/profile/rasgaard.com">Bluesky</a> if you also get excited about the possibilities of smaller models!</p>
<p>[^1]: Translates to &quot;I fucking love tebirkes&quot;. Tebirkes is a Danish poppy seed pastry. It has the transcription id 206 in the dataset.</p>

        </article>
    </main>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-markdown.min.js"></script>
    
    
    <script>
        // Theme toggle functionality
        const theme = localStorage.getItem('theme') || 'light';
        document.documentElement.setAttribute('data-theme', theme);

        function toggleTheme() {
            const current = document.documentElement.getAttribute('data-theme');
            const next = current === 'light' ? 'dark' : 'light';
            document.documentElement.setAttribute('data-theme', next);
            localStorage.setItem('theme', next);
        }

        // Add keyboard shortcut (Ctrl+T or Cmd+T)
        document.addEventListener('keydown', (e) => {
            if ((e.ctrlKey || e.metaKey) && e.key === 't') {
                e.preventDefault();
                toggleTheme();
            }
        });
    </script>
</body>

</html>